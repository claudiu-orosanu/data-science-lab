{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Android Data from PlayStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "%run ./Preprocessing.ipynb\n",
    "df = get_data();\n",
    "orig_df = df.copy();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Science\n",
    "\n",
    "We transformed the problem in a classification one. Now rating can be *poor* (< 4) and *excellent* (>=4). \n",
    "\n",
    "We achieved the following : \n",
    "- 70% with RandomForest(n_estimators = 100)\n",
    "\n",
    "Notes : \n",
    "- RandomForest clearly tends to overfit. Reducing the complexity of the tree algorithm doesn't improve accuracy in cross validation by no means. \n",
    "    + this may mean that the model is too complex. reducing the number of features took into account can help\n",
    "**Next thing** : We should try adding or changing the features of data, and try more values for the hyperparameters of the algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = orig_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.columns.values.tolist()\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_features = ['category', 'size', 'type', 'price', 'content_rating', 'genres', 'android_version', 'name_wc']\n",
    "post_features = [feature for feature in df.columns.values if feature not in pre_features]\n",
    "log_features = ['reviews', 'installs', 'name_wc', 'size', 'rating']\n",
    "cat_features = ['category', 'type', 'content_rating', 'genres']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_categories = True\n",
    "if use_categories:\n",
    "    pre_features = list(set(pre_features + cat_features))\n",
    "    post_features = list(set(post_features + cat_features))\n",
    "else:\n",
    "    pre_features = [ft for ft in pre_features if ft not in cat_features]\n",
    "    post_features = [ft for ft in post_features if ft not in cat_features]\n",
    "print('Predictors are : %s' % pre_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_log = False\n",
    "if use_log:\n",
    "    df[log_features] = np.log(df[log_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(df, test_size = 0.2, random_state = 42)\n",
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "Missing values and outliers removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from OutlierIQR import OutlierIQR\n",
    "to_remove_outliers = False\n",
    "outliers_cols = ['installs']\n",
    "if to_remove_outliers:\n",
    "    odetector = OutlierIQR()\n",
    "    odetector.fit(train, columns = ['installs'])\n",
    "    train = odetector.transform(train)\n",
    "    test = odetector.transform(test)\n",
    "train.shape\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Impute missing values using the median so we will not treat them as outliers later\n",
    "from sklearn.impute import SimpleImputer\n",
    "imp = SimpleImputer(missing_values = -1, strategy='median')\n",
    "missing_features = ['android_version', 'size']\n",
    "imp.fit(train[missing_features])\n",
    "train.loc[:, missing_features] = imp.transform(train[missing_features])\n",
    "test.loc[:, missing_features] = imp.transform(test[missing_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting machine learning\n",
    "Split data into predictors and labels, both for train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_samples_labels(df, label_column, keep_features = None):\n",
    "    Y = df[label_column].values\n",
    "    X = df.drop(columns = label_column)\n",
    "    if keep_features is not None:\n",
    "        X = X[keep_features]\n",
    "    bins = [Y.min(), np.percentile(Y, 70), Y.max()]\n",
    "    Y[Y < bins[1]] = 0\n",
    "    Y[Y >= bins[1]] = 1\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = split_samples_labels(train, 'installs', keep_features=pre_features)\n",
    "x_test, y_test = split_samples_labels(test, 'installs', keep_features=pre_features)\n",
    "x_train.shape\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional enhancements\n",
    "Oversampling and scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_train[y_train==1])\n",
    "len(y_train[y_train==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataset is rather imbalanced, which will skew the results. So we reduce the number of big rating examples\n",
    "# we can also try upsampling the small rating examples\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "upsample = False\n",
    "downsample = False\n",
    "if upsample:\n",
    "    sampler = RandomOverSampler(random_state = 42)\n",
    "    x_train, y_train = sampler.fit_resample(x_train,y_train)\n",
    "elif downsample :\n",
    "    sampler = RandomUnderSampler(random_state=42)\n",
    "    x_train, y_train = sampler.fit_resample(x_train, y_train)\n",
    "len(y_train[y_train==1])\n",
    "len(y_train[y_train==0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "# scale data if needed. forests and trees don't need it. Others do. \n",
    "scale = True\n",
    "if scale:\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    scaler.fit(x_train)\n",
    "    x_train = scaler.transform(x_train)\n",
    "    x_test = scaler.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tuning models hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble, tree, svm, neighbors\n",
    "from sklearn.model_selection import cross_val_score\n",
    "# KNN - best is n_neighbors = 4\n",
    "# KNN is very much affected by random upsampling. because many entries are duplicates\n",
    "knn = neighbors.KNeighborsClassifier()\n",
    "scores = []\n",
    "fit_grid = False\n",
    "if fit_grid:\n",
    "    try_neighbors = range(1,11)\n",
    "    for n_neighbors in try_neighbors:\n",
    "        knn.n_neighbors = n_neighbors;\n",
    "        val_scores = cross_val_score(knn, x_train, y_train, cv = 3 )\n",
    "        scores.append(val_scores.mean())\n",
    "    plt.plot(try_neighbors, scores, 'b*-');\n",
    "else:\n",
    "    knn.n_neighbors = 4\n",
    "    print('Validation score : %.2f' % cross_val_score(knn, x_train, y_train, cv = 3).mean());\n",
    "    knn.fit(x_train, y_train);\n",
    "    print('Test score : %.2f' % knn.score(x_test, y_test));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc = svm.SVC(kernel = 'rbf', random_state=42, probability = True,\n",
    "             C = 1, tol = .001, gamma = 9)\n",
    "fit_grid = False\n",
    "if fit_grid:\n",
    "    params = [{'kernel' : ['rbf'],\n",
    "            'gamma' : ['scale'],\n",
    "            'C' : [1],\n",
    "            'tol' : [0.001],\n",
    "            'gamma' : [9]}]\n",
    "    grid = GridSearchCV(estimator = svc,\n",
    "                  param_grid = params,\n",
    "                  cv = 3, iid = False)\n",
    "    grid.fit(x_train, y_train)\n",
    "    grid.score(x_test, y_test)\n",
    "else :\n",
    "    print('Validation score %.2f' % cross_val_score(svc, x_train, y_train, cv = 5).mean())\n",
    "    svc.fit(x_train, y_train)\n",
    "    print('Test score %.2f' % svc.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf =  ensemble.RandomForestClassifier(n_estimators=30, max_features=2, max_depth=12, min_samples_split=5, random_state=42)\n",
    "grid_fit = False\n",
    "if grid_fit:\n",
    "    params = {\n",
    "        'max_features' : [2, 4, 5],\n",
    "        'max_depth' : [12], # 10, 8, 18],\n",
    "        'min_samples_split' : [5], #, 2, 7, 10],\n",
    "        'n_estimators': [30, 20, 40]\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        estimator = rf, \n",
    "        param_grid = params,\n",
    "        cv = 5, \n",
    "        iid = False, \n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    grid.score(x_test, y_test)\n",
    "    print('Best score: %f' % grid.best_score_)\n",
    "    print('Best params: %s' % grid.best_params_)\n",
    "else:\n",
    "    rf.fit(x_train, y_train)\n",
    "    rf.score(x_train, y_train)\n",
    "    rf.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = tree.DecisionTreeClassifier(max_depth=9, min_impurity_decrease=0, min_samples_leaf=8, max_features=3, random_state=42)\n",
    "grid_fit = False\n",
    "if grid_fit:\n",
    "    params = {\n",
    "        'max_features': [1, 2, 3, 4, 5, 6, 7, 8],\n",
    "        'min_samples_leaf': [2, 3, 4, 5],\n",
    "        'min_impurity_decrease': [0, 0.01]\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        estimator = dt,\n",
    "        param_grid = params,\n",
    "        cv = 5, \n",
    "        iid = False, \n",
    "        return_train_score=True\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    grid.score(x_test, y_test)\n",
    "    print('Best score: %f' % grid.best_score_)\n",
    "    print('Best params: %s' % grid.best_params_)\n",
    "else:\n",
    "    dt.fit(x_train, y_train)\n",
    "    dt.score(x_train, y_train)\n",
    "    dt.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ada = ensemble.AdaBoostClassifier(\n",
    "    n_estimators=80,\n",
    "    base_estimator=None, # decision tree, by default\n",
    "    random_state = 42\n",
    ")\n",
    "grid_fit = False\n",
    "if grid_fit:\n",
    "    params = {\n",
    "        'n_estimators' : [20, 50, 80, 100],\n",
    "        'learning_rate' : [1, 2, 3, 0.8]\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        estimator = ada,\n",
    "        param_grid = params,\n",
    "        cv = 5, \n",
    "        iid = False, \n",
    "        return_train_score=True,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    grid.score(x_test, y_test)\n",
    "    print('Best score: %f' % grid.best_score_)\n",
    "    print('Best params: %s' % grid.best_params_)\n",
    "else:\n",
    "    ada.fit(x_train, y_train)\n",
    "    ada.score(x_train, y_train)\n",
    "    ada.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbc = ensemble.GradientBoostingClassifier(\n",
    "    n_estimators=400, \n",
    "    loss='deviance', \n",
    "    learning_rate=0.15, \n",
    "    subsample=0.9, \n",
    "    random_state = 42\n",
    ")\n",
    "grid_fit = False\n",
    "if grid_fit:\n",
    "    params = {\n",
    "        'n_estimators' : [100, 200, 300, 400],\n",
    "        'learning_rate' : [0.1, 0.15, 0.2, 0.25],\n",
    "        'loss': ['deviance', 'exponential'],\n",
    "        'subsample': [1.0, 0.5, 0.8, 0.85, 0.9, 0.95]\n",
    "    }\n",
    "    grid = GridSearchCV(\n",
    "        estimator = gbc,\n",
    "        param_grid = params,\n",
    "        cv = 5,\n",
    "        iid = False, \n",
    "        return_train_score=True, \n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(x_train, y_train)\n",
    "    grid.score(x_test, y_test)\n",
    "    print('Best score: %f' % grid.best_score_)\n",
    "    print('Best params: %s' % grid.best_params_)\n",
    "else:\n",
    "    gbc.fit(x_train, y_train)\n",
    "    gbc.score(x_train, y_train)\n",
    "    gbc.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = ensemble.BaggingClassifier(\n",
    "    base_estimator=None, # decision tree, by default\n",
    "    n_estimators=300,\n",
    "    max_samples=100,\n",
    "    max_features=6,\n",
    "    random_state = 42\n",
    ")\n",
    "grid_fit = False\n",
    "if grid_fit:\n",
    "    params = {\n",
    "        'n_estimators' : [20, 50, 80, 100, 200],\n",
    "        'max_samples' : [1, 2, 3, 5],\n",
    "        'max_features': [1, 2, 3, 5]\n",
    "    }\n",
    "    grid = GridSearchCV(estimator = bag,\n",
    "                  param_grid = params,\n",
    "                  cv = 5, iid = False, return_train_score=True, n_jobs=-1)\n",
    "    grid.fit(x_train, y_train)\n",
    "    grid.score(x_test, y_test)\n",
    "    grid.best_params_\n",
    "else:\n",
    "    bag.fit(x_train, y_train)\n",
    "    bag.score(x_train, y_train)\n",
    "    bag.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xgboost\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb = xgboost.XGBClassifier(\n",
    "    n_estimators=320,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    n_jobs=-1,\n",
    "    gamma=0,\n",
    "    min_child_weight=1,\n",
    "#     max_delta_step=0,\n",
    "    subsample=1,\n",
    "    random_state = 42\n",
    ")\n",
    "grid_fit = False\n",
    "if grid_fit:\n",
    "    params = {\n",
    "        'n_estimators' : [20, 50, 80, 100, 200, 300, 320],\n",
    "        'max_depth' : [1, 2, 3, 4, 5],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.15],\n",
    "        'gamma': [0, 1, 2]\n",
    "    }\n",
    "    grid = GridSearchCV(estimator = xgb,\n",
    "                  param_grid = params,\n",
    "                  cv = 5, iid = False, return_train_score=True, n_jobs=-1)\n",
    "    grid.fit(x_train, y_train)\n",
    "    grid.score(x_test, y_test)\n",
    "    grid.best_params_\n",
    "else:\n",
    "    xgb.fit(x_train, y_train)\n",
    "    xgb.score(x_train, y_train)\n",
    "    xgb.score(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import Model, ModelsBenchmark\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "models = [svc, rf, dt, knn, ada, gbc, bag, xgb]\n",
    "# add voting method\n",
    "estimators = [ (model.__class__.__name__,model) for model in models]\n",
    "voting_clf = VotingClassifier(estimators=estimators, voting='soft', n_jobs=-1)\n",
    "models.append(voting_clf)\n",
    "# benchmark between all models so far\n",
    "bench = ModelsBenchmark(models);\n",
    "bench.fit(x_train, y_train)\n",
    "bench.score(x_train, y_train)\n",
    "bench.score(x_test, y_test)\n",
    "bench._scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce dimensionality to be able to plot data\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components = 2, random_state = 42);\n",
    "    \n",
    "def reduce_dimensions(X, fit=False):\n",
    "    if fit:\n",
    "        pca.fit(X)\n",
    "    return pca.transform(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and plot metrics for the best one\n",
    "from sklearn.metrics import confusion_matrix\n",
    "bench.fit(x_train, y_train)\n",
    "bench.score(x_test, y_test)\n",
    "clf = bench._sorted[0]\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 4);\n",
    "fig.subplots_adjust(right = 2);\n",
    "x_plot = reduce_dimensions(x_train, fit = True)\n",
    "axs[0].scatter(x_plot[:, 0], x_plot[:,1], c = y_train);\n",
    "axs[0].set_title('Train Data');\n",
    "axs[0].set_xlabel('x_0');\n",
    "axs[0].set_ylabel('x_1');\n",
    "x_plot = reduce_dimensions(x_test)\n",
    "axs[1].scatter(x_plot[:, 0], x_plot[:,1], c = y_test);\n",
    "axs[1].set_title('Test Data');\n",
    "axs[1].set_xlabel('x_0');\n",
    "axs[1].set_ylabel('x_1');\n",
    "test_cnf_matrix = confusion_matrix(y_test, clf.predict(x_test))\n",
    "sns.heatmap(test_cnf_matrix, ax = axs[2], vmin = 0);\n",
    "axs[2].set_title('Test');\n",
    "axs[2].set_xlabel('Predicted');\n",
    "axs[2].set_ylabel('Actual');\n",
    "train_cnf_matrix = confusion_matrix(y_train, clf.predict(x_train))\n",
    "sns.heatmap(train_cnf_matrix, ax = axs[3], vmin = 0);\n",
    "axs[3].set_title('Train');\n",
    "axs[3].set_xlabel('Predicted');\n",
    "axs[3].set_ylabel('Actual');\n",
    "print(train_cnf_matrix)\n",
    "print('Train Accuracy : %.2f ' % clf.score(x_train, y_train))\n",
    "print('Test Accuracy : %.2f ' % clf.score(x_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we make predictions on 2-dimensional data and plot the points labeled wrong.   \n",
    "Currently, below part has a bug and doesn't run correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to solve the bug below, set the max_features attribute to 2\n",
    "# rf_plot = rf\n",
    "# rf_plot.max_features = 2\n",
    "# dt_plot = dt\n",
    "# dt_plot.max_features = 2\n",
    "# vot_plot = voting_clf\n",
    "# vot_plot.estimators = [svc, knn, rf_plot, dt_plot]\n",
    "# clf = vot_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot both labels separately and our predictions on them\n",
    "fig, axs = plt.subplots(nrows = 1, ncols = 2)\n",
    "x_plot_train = reduce_dimensions(x_train, fit=True)\n",
    "y_plot_train = y_train\n",
    "x_plot_test = reduce_dimensions(x_test)\n",
    "y_plot_test = y_test\n",
    "clf.fit(x_plot_train,y_plot_train)\n",
    "x_plot = x_plot_test\n",
    "y_plot = y_plot_test\n",
    "y_pred = clf.predict(x_plot)\n",
    "fig.subplots_adjust(right = 2)\n",
    "labeled_0 = x_plot[y_plot == 0]\n",
    "scatter = axs[0].scatter(labeled_0[:,0], labeled_0[:, 1], c = y_pred[y_plot ==  0], )\n",
    "fig.colorbar(scatter, ax = axs[0])\n",
    "axs[0].set_title('Points with true label 0 ')\n",
    "labeled_0_acc = (y_pred == 0) == (y_plot == 0).sum() / len(y_pred)\n",
    "# print('Accuracy for true label 0 : %.3f' % labeled_0_acc)\n",
    "labeled_1 = x_plot[y_plot == 1]\n",
    "scatter = axs[1].scatter(labeled_1[:,0], labeled_1[:, 1], c = y_pred[y_plot == 1] )\n",
    "fig.colorbar(scatter, ax = axs[1])\n",
    "axs[1].set_title('Points with true label 1 ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print some correctly and incorrectly labeled data\n",
    "from random import randint\n",
    "clf.fit(x_train, y_train)\n",
    "y_pred = clf.predict(x_test)\n",
    "# correct = x_test[y_pred == y_test]\n",
    "# incorrect = x_test[~(y_pred == y_test)]\n",
    "\n",
    "columns = df.columns.values\n",
    "def get_samples(x, y_true, y_pred, sample_type = 'correct', count = 5):\n",
    "    mask = (y_pred == y_true)\n",
    "    if sample_type == 'incorrect':\n",
    "        mask = ~mask\n",
    "    x = x[mask]\n",
    "    y_pred = y_pred[mask]\n",
    "    y_true = y_true[mask]\n",
    "    df = pd.DataFrame(columns=[*columns , 'predicted', 'true'])\n",
    "    if len(x) == 0:\n",
    "        return df\n",
    "    for _ in range(count):\n",
    "        idx = randint(0, len(x)) \n",
    "        dct = {}\n",
    "        dct['predicted'] = y_pred[idx]\n",
    "        dct['true'] = y_true[idx]\n",
    "        for i in range(x.shape[1]):\n",
    "            dct[df.columns.values[i]] = x[idx][i]\n",
    "        df = df.append(dct,\n",
    "                       ignore_index=True)\n",
    "    return df\n",
    "\n",
    "print(\"====Correct samples =====\")\n",
    "get_samples(x_test, y_test, y_pred, sample_type='correct') \n",
    "print(\"====Incorrect samples =====\")\n",
    "get_samples(x_test, y_test, y_pred, sample_type='incorrect') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Visualisation of the decision tree created by the algorithm, for fun and insight\n",
    "import graphviz\n",
    "from sklearn.tree import export_graphviz\n",
    "clf = Model(dt)\n",
    "clf.compute_scores((x_train, y_train), (x_test, y_test))\n",
    "clf.model.tree_.max_depth\n",
    "dot_data = export_graphviz(clf.model,\n",
    "                           out_file=None,\n",
    "                           feature_names=pre_features,\n",
    "                           class_names=['fair', 'excellent'],\n",
    "                           filled=True,\n",
    "                           rounded=True)\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import layers\n",
    "\n",
    "num_classes = 2\n",
    "input_shape = x_train.shape[1]\n",
    "model = keras.Sequential()\n",
    "model.add(layers.Dense(512, activation='relu', input_shape=(input_shape,)))\n",
    "model.add(layers.BatchNormalization())\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1024, activation='relu'))\n",
    "# model.add(layers.BatchNormalization(input_shape=(input_shape,)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(512, activation = 'relu'))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(256, activation = 'relu'))\n",
    "model.add(layers.Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "            optimizer='rmsprop',\n",
    "            metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical\n",
    "no_epochs = 20\n",
    "batch_size = 1024\n",
    "history = model.fit(x_train, to_categorical(y_train), validation_split=.3, batch_size = batch_size, epochs = no_epochs )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.evaluate(x_test, to_categorical(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame({'epochs':history.epoch, 'loss': history.history['loss'], \n",
    "                   'validation_loss': history.history['val_loss']\n",
    "                  })\n",
    "g = sns.pointplot(x=\"epochs\", y=\"loss\", data=df, fit_reg=False, color = 'yellow')\n",
    "# g = sns.pointplot(x=\"epochs\", y=\"validation_loss\", data=df, fit_reg=False, color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "df = pd.DataFrame({'epochs':history.epoch, 'accuracy': history.history['acc']\n",
    "                   , 'validation_accuracy': history.history['val_acc']\n",
    "                  })\n",
    "g = sns.pointplot(x=\"epochs\", y=\"accuracy\", data=df, fit_reg=False)\n",
    "# g = sns.pointplot(x=\"epochs\", y=\"validation_accuracy\", data=df, fit_reg=False, color='green')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model voting \n",
    "Maybe below weights will help raise model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import ensemble, svm, neighbors\n",
    "from collections import Counter\n",
    "estimators = [\n",
    "#     ('svc', svm.SVC(kernel = 'rbf', random_state=42, probability = True, C = 1, tol = .001, gamma = 9)),\n",
    "    ('rf', ensemble.RandomForestClassifier(random_state = 42, max_features=3, max_depth=12, n_estimators=30, min_samples_split=5)),\n",
    "    ('knn', neighbors.KNeighborsClassifier(n_neighbors=2)),\n",
    "    ('ada', ensemble.AdaBoostClassifier(n_estimators=80, random_state = 42)),\n",
    "    ('gb', ensemble.GradientBoostingClassifier(n_estimators=400, loss='deviance', learning_rate=0.15, subsample=0.9, random_state = 42))\n",
    "]\n",
    "weights = [5, 10, 40, 140]\n",
    "voting_classifier = ensemble.VotingClassifier(estimators=estimators, voting='soft', weights=weights, n_jobs=-1)\n",
    "voting_classifier.fit(x_train, y_train)\n",
    "confidence = voting_classifier.score(x_test, y_test)\n",
    "predictions = voting_classifier.predict(x_test)\n",
    "print('accuracy: %s %s' % (confidence, weights))\n",
    "print('predictions:', Counter(predictions))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
